---
title: "Class 08 Mini Project"
author: "Jessica Diaz-Vigil"
format: pdf
---

## Getting Organized

First I will import the correct data.

```{r}
fna.data <- "https://marcos-diazg.github.io/BIMM143_SP23/class-material/class8/WisconsinCancer.csv"
```

```{r}
wisc.df <- read.csv(fna.data, row.names=1) 
head(wisc.df)
```

We need to remove the diagnosis. To do this we will remove the column.

```{r}
wisc.data <- wisc.df[,-1]
head(wisc.data)
```

We need to set up a new vector called `diagnosis` that contains the data from the diagnosis column, but we will store it as a factor.

```{r}
wisc.df <- read.csv(fna.data, row.names=1)
diagnosis <- wisc.df[,1]
```

# **Exploratory Data Analysis**

**Q1**. How many observations are in this dataset?

```{r}
wisc.df <- read.csv(fna.data, row.names=1)
length(wisc.df$diagnosis)
```

There were 569 observations in this dataset.

**Q2**. How many of the observations have a malignant diagnosis?

```{r}
wisc.df <- read.csv(fna.data, row.names=1)
table(diagnosis)
```

There were 212 observations which were malignant.

**Q3**. How many variables/features in the data are suffixed with `_mean`?

```{r}
wisc.df <- read.csv(fna.data, row.names=1)
library(stringr)
cn<-colnames(wisc.data, do.NULL = TRUE, prefix = "col")
sum(str_count(cn, "_mean"))
```

There are 10 variables in the data which are suffixed with `_mean`

# Principal Component Analysis

## Performing PCA

```{r}
round(colMeans(wisc.data), 2)
round(apply(wisc.data,2,sd), 2)
```

```{r}
wisc.pr <- prcomp( wisc.data, scale = T, center = T )
summary(wisc.pr)
```

-   **Q4**. From your results, what proportion of the original variance is captured by the first principal components (PC1)?

-   **Q5**. How many principal components (PCs) are required to describe at least 70% of the original variance in the data?

-   **Q6**. How many principal components (PCs) are required to describe at least 90% of the original variance in the data?

## Interpreting PCA Results

Creating a `biplot`:

```{r}
biplot(wisc.pr)
```

-   **Q7.** What stands out to you about this plot? Is it easy or difficult to understand? Why?

    This plot is very hard to understand since its so noisy and there are numbers everywhere that are comprehensible.

-   **Q8.** Generate a similar plot for principal components 1 and 3. What do you notice about these plots?

    ```{r}
    plot(wisc.pr$x[, c(1, 2)],
         xlab = "PC1", ylab = "PC2")
    ```

    ```{r}
    diagnosis <- as.numeric(wisc.df[,1])
    plot(wisc.pr$x[, c(1, 3)],
         xlab = "PC1", ylab = "PC3")
    ```

## Variance Explained

```{r}
pr.var <- wisc.pr$sdev^2
head(pr.var)
```

```{r}
pve <- pr.var/sum(pr.var)
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

```{r}
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
```

```{r}
plot(cumsum(pve), xlab = "Principal Component", 
     ylab = "Cumulative Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
```

## Communicating PCA Results

-   **Q9.** For the first principal component, what is the component of the loading vector (i.e. `wisc.pr$rotation[,1]`) for the feature `concave.points_mean`?

    ```{r}
    wisc.pr$rotation["concave.points_mean", 1]
    ```

This gives us a value of -0.26 which means that this is the value for that mean.

-   **Q10.** What is the minimum number of principal components required to explain 80% of the variance of the data?

    ```{r}
    plot(cumsum(pve), xlab = "Principal Component", 
         ylab = "Cumulative Proportion of Variance Explained", 
         ylim = c(0, 1), type = "b")
    ```

We can see that above the horizontal line for 0.8 is attributed for the top 5 square sums

# **Hierarchical Clustering of Case Data**

```{r}
data.scaled <- scale(wisc.data)
```

```{r}
data.dist <- dist(data.scaled)
```

```{r}
wisc.hclust <- hclust(data.dist, method = "complete")
```

# Results of Hierarchical Clustering

-   **Q11.** Using the `plot()` function, what is the height at which the clustering model has 4 clusters?

    ```{r}
    plot(wisc.hclust)
    ```

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, h=20)
unique(wisc.hclust.clusters)
```

-   **Q12.** Can you find a better cluster vs diagnoses match with by cutting into a different number of clusters between 2 and 10?

```{r}
wisc.hclust.clusters <- cutree(wisc.hclust, k = 4)
table(wisc.hclust.clusters, diagnosis)
```

# K-means Clustering and Comparing Results

```{r}
wisc.km <- kmeans(data.scaled, centers= 2, nstart= 20)
```

```{r}
table(wisc.km$cluster, diagnosis)
```

-   **Q13**. How well does k-means separate the two diagnoses? How does it compare to your hclust results?

```{r}
table(wisc.hclust.clusters, wisc.km$cluster)
sum(apply(table(wisc.hclust.clusters, wisc.km$cluster), 1, min))
```

# Clustering on PCA Results

```{r}
wisc.pr.hclust <- hclust(data.dist, method="complete")
```

```{r}
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=4)
```

-   **Q14**. How well does the newly created model with four clusters separate out the two diagnoses?

```{r}
table(wisc.pr.hclust.clusters, diagnosis)
```

-   **Q15**. How well do the k-means and hierarchical clustering models you created in previous sections (i.e. before PCA) do in terms of separating the diagnoses? Again, use the `table()` function to compare the output of each model (`wisc.km$cluster` and `wisc.hclust.clusters`) with the vector containing the actual diagnoses.

```{r}
table(wisc.km$cluster, diagnosis)
table(wisc.hclust.clusters, diagnosis)
```

-   **Q16.** Which of your analysis procedures resulted in a clustering model with the best specificity? How about sensitivity?

I think the second one may be best
